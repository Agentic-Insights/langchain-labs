{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Tutorial: Loading Prompts from Files & Using RunnableSequence\n",
    "\n",
    "## Welcome to the LangChain RunnableSequence Tutorial!\n",
    "\n",
    "In this notebook, we'll explore LangChain, focusing on loading prompts from files and using PromptTemplates. Since we are focusing on file reading, we must ensure we have a sample file separate from the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab Notes\n",
    "\n",
    "ðŸ›‘ Google Colab users must upload to the Colab environment once opened up.\n",
    "\n",
    "1. Open the Colab - [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Agentic-Insights/langchain-labs/blob/main/labs/002-tutorial.ipynb)\n",
    "2. Click on the folder on the left bar - \"Files\"\n",
    "3. Click on the Upload file icon and upload a sample markdown file named `002-tutorial.prompt.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the OpenAI API key from the environment variables\n",
    "load_dotenv()\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    # If the key is not found in the environment variables, try to get it from Google Colab userdata\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "    if os.environ[\"OPENAI_API_KEY\"] is None:\n",
    "        print(\"OPENAI_API_KEY not found in environment variables or Google Colab userdata.\")\n",
    "else:\n",
    "    print(\"OpenAI API key found in environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our language model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in a file as the prompt to show dynamic prompting as the next step\n",
    "\n",
    "Let's break down what's happening here:\n",
    "\n",
    "1. We load the content of prompt.md into the variable `prompt_template`.\n",
    "1. We create a ChatPromptTemplate from this content using `ChatPromptTemplate.from_template(prompt_template)`.\n",
    "1. The `chat_prompt.format(topic=topic)` line shows exactly how the template replaces the `{topic}` placeholder with the actual topic. This is what's happening behind the scenes when we use the template in our chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load prompt from file\n",
    "def load_prompt_template(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load the prompt template from file\n",
    "prompt_template = load_prompt_template('002-tutorial.prompt.md')\n",
    "# Create a ChatPromptTemplate\n",
    "chat_prompt = ChatPromptTemplate.from_template(prompt_template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding RunnableSequence in LangChain\n",
    "\n",
    "## What is RunnableSequence?\n",
    "\n",
    "RunnableSequence is a fundamental concept in LangChain that allows you to chain together multiple components in a sequential manner. It's a powerful tool for creating complex workflows with language models.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "1. **Composability**: Easily combine different LangChain components.\n",
    "2. **Flexibility**: Works with various types of components (prompts, models, tools, etc.).\n",
    "3. **Readability**: Creates clear, linear workflows.\n",
    "4. **Reusability**: Sequences can be saved and reused in different parts of your application.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "A RunnableSequence takes multiple components and runs them in order, passing the output of one component as input to the next.\n",
    "\n",
    "## Basic Structure\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "sequence = RunnableSequence(\n",
    "    component1,\n",
    "    component2,\n",
    "    component3\n",
    ")\n",
    "```\n",
    "\n",
    "## Common Use Case\n",
    "\n",
    "A typical use case is combining a prompt template with a language model:\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "# Create components\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a fact about {topic}\")\n",
    "llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "\n",
    "# Create sequence\n",
    "sequence = RunnableSequence(\n",
    "    prompt,\n",
    "    llm\n",
    ")\n",
    "\n",
    "# Use the sequence\n",
    "result = sequence.invoke({\"topic\": \"space\"})\n",
    "print(result.content)\n",
    "```\n",
    "\n",
    "## Advanced Usage\n",
    "\n",
    "RunnableSequence can be used for more complex workflows:\n",
    "\n",
    "1. **Data Preprocessing**: Add a step to clean or format input data.\n",
    "2. **Post-processing**: Modify the output of the language model.\n",
    "3. **Branching Logic**: Use conditional statements to determine the next step.\n",
    "4. **Tool Integration**: Incorporate external tools or APIs into your sequence.\n",
    "\n",
    "## Benefits of Using RunnableSequence\n",
    "\n",
    "1. **Modularity**: Easily swap out components to experiment with different approaches.\n",
    "2. **Scalability**: Start simple and gradually add complexity as needed.\n",
    "3. **Maintainability**: Clear structure makes it easier to understand and modify workflows.\n",
    "4. **Performance**: Efficient execution of complex chains of operations.\n",
    "\n",
    "RunnableSequence is a cornerstone of building sophisticated applications with LangChain, allowing you to create powerful, flexible, and maintainable AI-powered workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RunnableSequence\n",
    "chain = RunnableSequence(\n",
    "    chat_prompt,\n",
    "    llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When we invoke the chain with chain.invoke({\"topic\": topic}), it's using this formatted prompt to generate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the chain\n",
    "response = chain.invoke({\"topic\": \"artificial intelligence\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try another topic\n",
    "response = chain.invoke({\"topic\": \"space exploration\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook demonstrates how PromptTemplates work with file input in LangChain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cod-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
