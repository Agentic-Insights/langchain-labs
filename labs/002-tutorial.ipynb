{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Tutorial: Loading Prompts from Files & Using RunnableSequence\n",
    "\n",
    "## Welcome to the LangChain RunnableSequence Tutorial!\n",
    "\n",
    "In this notebook, we'll explore LangChain, focusing on loading prompts from files and using PromptTemplates. Since we are focusing on file reading, we must ensure we have a sample file separate from the code.\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Agentic-Insights/langchain-labs/blob/main/labs/002-tutorial.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (0.2.10)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (0.1.17)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.22 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from langchain) (0.2.22)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from langchain) (0.1.93)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.32.0 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from langchain-openai) (1.37.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.22->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.22->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain-openai) (1.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.22->langchain) (3.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vaski\\desktop\\langchain-labs\\.conda\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.32.0->langchain-openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab support\n",
    "The next section pulls the required artifact '002-tutorial.prompt.md' from GitHub and puts it in the current directory for Colab if it is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "002-tutorial.prompt.md found in the current directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the file name to check\n",
    "file_name = '002-tutorial.prompt.md'\n",
    "# Define the repository URL to clone if the file is not found\n",
    "repo_url = 'https://github.com/Agentic-Insights/langchain-labs'\n",
    "\n",
    "# Check if the file exists in the current directory\n",
    "if not os.path.isfile(file_name):\n",
    "    print(f\"{file_name} not found. Cloning the repository.\")\n",
    "    # Use the `!` command to clone the repository in Google Colab\n",
    "    !git clone {repo_url}\n",
    "    !cp langchain-labs/labs/{file_name} .\n",
    "else:\n",
    "    print(f\"{file_name} found in the current directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key found in environment variables.\n"
     ]
    }
   ],
   "source": [
    "# Load the OpenAI API key from the environment variables\n",
    "load_dotenv()\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    # If the key is not found in the environment variables, try to get it from Google Colab userdata\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "    if os.environ[\"OPENAI_API_KEY\"] is None:\n",
    "        print(\"OPENAI_API_KEY not found in environment variables or Google Colab userdata.\")\n",
    "else:\n",
    "    print(\"OpenAI API key found in environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our language model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in a file as the prompt to show dynamic prompting as the next step\n",
    "\n",
    "Let's break down what's happening here:\n",
    "\n",
    "1. We load the content of prompt.md into the variable `prompt_template`.\n",
    "1. We create a ChatPromptTemplate from this content using `ChatPromptTemplate.from_template(prompt_template)`.\n",
    "1. The `chat_prompt.format(topic=topic)` line shows exactly how the template replaces the `{topic}` placeholder with the actual topic. This is what's happening behind the scenes when we use the template in our chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load prompt from file\n",
    "def load_prompt_template(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load the prompt template from file\n",
    "prompt_template = load_prompt_template('002-tutorial.prompt.md')\n",
    "# Create a ChatPromptTemplate\n",
    "chat_prompt = ChatPromptTemplate.from_template(prompt_template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding RunnableSequence in LangChain\n",
    "\n",
    "## What is RunnableSequence?\n",
    "\n",
    "RunnableSequence is a fundamental concept in LangChain that allows you to chain together multiple components in a sequential manner. It's a powerful tool for creating complex workflows with language models.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "1. **Composability**: Easily combine different LangChain components.\n",
    "2. **Flexibility**: Works with various types of components (prompts, models, tools, etc.).\n",
    "3. **Readability**: Creates clear, linear workflows.\n",
    "4. **Reusability**: Sequences can be saved and reused in different parts of your application.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "A RunnableSequence takes multiple components and runs them in order, passing the output of one component as input to the next.\n",
    "\n",
    "## Basic Structure\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "sequence = RunnableSequence(\n",
    "    component1,\n",
    "    component2,\n",
    "    component3\n",
    ")\n",
    "```\n",
    "\n",
    "## Common Use Case\n",
    "\n",
    "A typical use case is combining a prompt template with a language model:\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "# Create components\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a fact about {topic}\")\n",
    "llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "\n",
    "# Create sequence\n",
    "sequence = RunnableSequence(\n",
    "    prompt,\n",
    "    llm\n",
    ")\n",
    "\n",
    "# Use the sequence\n",
    "result = sequence.invoke({\"topic\": \"space\"})\n",
    "print(result.content)\n",
    "```\n",
    "\n",
    "## Advanced Usage\n",
    "\n",
    "RunnableSequence can be used for more complex workflows:\n",
    "\n",
    "1. **Data Preprocessing**: Add a step to clean or format input data.\n",
    "2. **Post-processing**: Modify the output of the language model.\n",
    "3. **Branching Logic**: Use conditional statements to determine the next step.\n",
    "4. **Tool Integration**: Incorporate external tools or APIs into your sequence.\n",
    "\n",
    "## Benefits of Using RunnableSequence\n",
    "\n",
    "1. **Modularity**: Easily swap out components to experiment with different approaches.\n",
    "2. **Scalability**: Start simple and gradually add complexity as needed.\n",
    "3. **Maintainability**: Clear structure makes it easier to understand and modify workflows.\n",
    "4. **Performance**: Efficient execution of complex chains of operations.\n",
    "\n",
    "RunnableSequence is a cornerstone of building sophisticated applications with LangChain, allowing you to create powerful, flexible, and maintainable AI-powered workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RunnableSequence\n",
    "chain = RunnableSequence(\n",
    "    chat_prompt,\n",
    "    llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When we invoke the chain with chain.invoke({\"topic\": topic}), it's using this formatted prompt to generate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I have successfully loaded the prompt.md file for this tutorial. Here’s a haiku about artificial intelligence:\n",
      "\n",
      "Silent circuits hum,  \n",
      "Learning, growing in the dark,  \n",
      "Mind of code takes flight.\n"
     ]
    }
   ],
   "source": [
    "# Use the chain\n",
    "response = chain.invoke({\"topic\": \"artificial intelligence\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I've successfully loaded the prompt.md file for this tutorial. Here’s a haiku about space exploration:\n",
      "\n",
      "Stars whisper secrets,  \n",
      "Vastness calls the brave to roam,  \n",
      "Beyond our blue home.\n"
     ]
    }
   ],
   "source": [
    "# Let's try another topic\n",
    "response = chain.invoke({\"topic\": \"space exploration\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook demonstrates how PromptTemplates work with file input in LangChain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cod-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
